{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Heuristic Policy\n",
    "In this section, we will walk through how to define a POMDP policy of your own. For more details on POMDPs and their policies, please consult Chapter 6 of the DMU textbook [1] We will define a simple greedy policy that takes the action that maximises the expected single-step reward, given the current belief state. We will also compare it against a policy that chooses actions at random. Please look at the documentation of [POMDPPolicies.jl](https://github.com/JuliaPOMDP/POMDPPolicies.jl) for more on the code structure of a policy object that is compatible with POMDPs.jl. We will use the explicit TigerPOMDP model - see [this](http://localhost:8888/notebooks/POMDPExamples/notebooks/Defining-a-POMDP-with-the-Explicit-Interface.ipynb) notebook for more on that.\n",
    "\n",
    "[1] Kochenderfer, Mykel J. Decision Making Under Uncertainty: Theory and Application. MIT Press, 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using POMDPs\n",
    "using POMDPPolicies # For defining a policy\n",
    "using POMDPModels # For the TigerPOMDP Model\n",
    "using BeliefUpdaters # To use DiscreteUpdater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a GreedyPolicy type that only requires the POMDP instance and the set of valid actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct GreedyPolicy{P<:POMDP, A} <: Policy\n",
    "    pomdp::P\n",
    "    action_map::Vector{A}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GreedyPolicy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function GreedyPolicy(pomdp::POMDP)\n",
    "    GreedyPolicy(pomdp, actions(pomdp))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overriding the POMDPs.jl action function\n",
    "Now we will define a new method for the `action` function, which specifies the behavior of our policy. It requires the belief state to be represented as a `DiscreteBelief`, i.e. a Probability Mass Function over individual states. It computes the expected single-step reward for each action, given the current belief state, and chooses the maximum one. This is sometimes called a \"greedy\" or \"myopic\" policy. Note that we must use `POMDPs.action` to add a method to the `action` function of `POMDPs.jl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.action(p::GreedyPolicy, b::DiscreteBelief)\n",
    "    max_value = -Inf\n",
    "    best_idx = 1\n",
    "    for i = 1:n_actions(p.pomdp)\n",
    "        a = p.action_map[i]\n",
    "        action_val = 0.0\n",
    "        for (bel,state) in zip(b.b, b.state_list)\n",
    "            action_val += bel*reward(p.pomdp, state, a)\n",
    "        end\n",
    "        \n",
    "        if action_val > max_value\n",
    "            best_idx = i\n",
    "            max_value = action_val\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return p.action_map[best_idx]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking the Policy\n",
    "Note that unlike the other examples on using *solvers*, here we have already specified a *policy*. Therefore, we can just evaluate the policy on a problem, in this case, `TigerPOMDP`(defined in [POMDPModels](https://github.com/JuliaPOMDP/POMDPModels.jl)). We define the POMDP problem and create the policies based on it.\n",
    "\n",
    "Since we only care about the discounted reward, we can use the rollout simulator defined in [POMDPSimulators](https://github.com/JuliaPOMDP/POMDPSimulators.jl). Checkout this [notebook](https://github.com/JuliaPOMDP/POMDPExamples.jl/blob/master/notebooks/Running-Simulations.ipynb) for ways to use the other simulators as well. Finally, we can compare the expected discounted rewards and see how the greedy policy does quite better than random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TigerPOMDP(-1.0, -100.0, 10.0, 0.85, 0.95)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pomdp = TigerPOMDP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GreedyPolicy{TigerPOMDP,Int64}(TigerPOMDP(-1.0, -100.0, 10.0, 0.85, 0.95), [0, 1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_pol = GreedyPolicy(pomdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a random policy as a benchmark\n",
    "rand_policy = RandomPolicy(pomdp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/shushman/.julia/compiled/v1.0/POMDPSimulators/i1HOp.ji for POMDPSimulators [e0d0a172-29c6-5d4e-96d0-f262df5d01fd]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    }
   ],
   "source": [
    "using POMDPSimulators\n",
    "rollout_sim = RolloutSimulator(max_steps=10);\n",
    "history_greedy = simulate(rollout_sim, pomdp, greedy_pol, DiscreteUpdater(pomdp));\n",
    "history_rand = simulate(rollout_sim, pomdp, rand_policy, DiscreteUpdater(pomdp));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history_greedy = 8.616017791798825\n",
      "history_rand = -411.2217274496074\n"
     ]
    }
   ],
   "source": [
    "@show history_greedy;\n",
    "@show history_rand;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.4",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
